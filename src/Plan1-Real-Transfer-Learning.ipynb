{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mode\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from heapq import * \n",
    "import time\n",
    "import copy\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "from skimage import io\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import pytorch_model_summary as pms\n",
    "from torchviz import make_dot\n",
    "np.random.seed(42)\n",
    "\n",
    "# import tensorflow as tf\n",
    "NUM_CLASS = 2\n",
    "WINDOW = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir=None, transform=None):\n",
    "        path = os.path.join(root_dir,'labels.csv')\n",
    "        self.ylabels = pd.read_csv(path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ylabels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_len = 5\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    " \n",
    "        label = self.ylabels.iloc[idx,1]\n",
    "        label_str = str(label) + '/'\n",
    "        img_folder = os.path.join(self.root_dir, label_str)\n",
    "        \n",
    "        img_arr = np.zeros((5, 1, 64, 64))\n",
    "        for i in range (5):\n",
    "            img_name = img_folder + str(idx) + '-' + str(i) + '.jpg'\n",
    "            image = io.imread(img_name)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            img_arr[i][0] = image\n",
    "            \n",
    "        label_1hot = np.zeros(NUM_CLASS)\n",
    "        label_1hot[label] = 1.0\n",
    "        label_1hot = torch.Tensor(label_1hot)\n",
    "#         label = np.array([label])\n",
    "        label = torch.LongTensor(np.array([label]))\n",
    "        sample = {'images': img_arr, 'label': label_1hot}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     5,
     10,
     46
    ]
   },
   "outputs": [],
   "source": [
    "class Reshape1(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = torch.Size([1,5,1,64,64])\n",
    "        return x.view(batch_size, -1)\n",
    "    \n",
    "class Reshape2(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = torch.Size([1,1,64,64])\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "def build_encoder():\n",
    "    encoder = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=192, kernel_size=3),\n",
    "        nn.BatchNorm2d(num_features=192),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=192, out_channels=192, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Conv2d(in_channels=192, out_channels=192, kernel_size=3, stride=2),\n",
    "        nn.BatchNorm2d(num_features=192),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        \n",
    "        nn.Conv2d(in_channels=192, out_channels=96, kernel_size=3),\n",
    "        nn.BatchNorm2d(num_features=96),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=96, out_channels=96, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=96, out_channels=96, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.Dropout(p=0.5),\n",
    "        \n",
    "        nn.Conv2d(in_channels=96, out_channels=32, kernel_size=3),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=2),\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return encoder\n",
    "\n",
    "def build_decoder():\n",
    "    decoder = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=32, out_channels=NUM_CLASS, kernel_size=1),\n",
    "        nn.AdaptiveAvgPool2d((1,1)),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_encoder = encoder\n",
    "        self.in_decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tmp = self.in_encoder(x)\n",
    "        y = self.in_decoder(tmp)\n",
    "        return y\n",
    "    def encoder(self, x):\n",
    "        return self.in_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "           Layer (type)         Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "               Conv2d-1     [1, 192, 62, 62]           1,920           1,920\n",
      "          BatchNorm2d-2     [1, 192, 62, 62]             384             384\n",
      "                 ReLU-3     [1, 192, 62, 62]               0               0\n",
      "               Conv2d-4     [1, 192, 62, 62]          37,056          37,056\n",
      "                 ReLU-5     [1, 192, 62, 62]               0               0\n",
      "               Conv2d-6     [1, 192, 30, 30]         331,968         331,968\n",
      "          BatchNorm2d-7     [1, 192, 30, 30]             384             384\n",
      "                 ReLU-8     [1, 192, 30, 30]               0               0\n",
      "              Dropout-9     [1, 192, 30, 30]               0               0\n",
      "              Conv2d-10      [1, 96, 28, 28]         165,984         165,984\n",
      "         BatchNorm2d-11      [1, 96, 28, 28]             192             192\n",
      "                ReLU-12      [1, 96, 28, 28]               0               0\n",
      "              Conv2d-13      [1, 96, 28, 28]           9,312           9,312\n",
      "                ReLU-14      [1, 96, 28, 28]               0               0\n",
      "              Conv2d-15      [1, 96, 28, 28]           9,312           9,312\n",
      "                ReLU-16      [1, 96, 28, 28]               0               0\n",
      "           MaxPool2d-17      [1, 96, 13, 13]               0               0\n",
      "             Dropout-18      [1, 96, 13, 13]               0               0\n",
      "              Conv2d-19      [1, 32, 11, 11]          27,680          27,680\n",
      "         BatchNorm2d-20      [1, 32, 11, 11]              64              64\n",
      "                ReLU-21      [1, 32, 11, 11]               0               0\n",
      "              Conv2d-22        [1, 32, 9, 9]           9,248           9,248\n",
      "         BatchNorm2d-23        [1, 32, 9, 9]              64              64\n",
      "                ReLU-24        [1, 32, 9, 9]               0               0\n",
      "              Conv2d-25      [1, 32, 11, 11]           9,248           9,248\n",
      "         BatchNorm2d-26      [1, 32, 11, 11]              64              64\n",
      "                ReLU-27      [1, 32, 11, 11]               0               0\n",
      "              Conv2d-28       [1, 2, 11, 11]              66              66\n",
      "   AdaptiveAvgPool2d-29         [1, 2, 1, 1]               0               0\n",
      "             Softmax-30         [1, 2, 1, 1]               0               0\n",
      "=============================================================================\n",
      "Total params: 602,946\n",
      "Trainable params: 602,946\n",
      "Non-trainable params: 0\n",
      "Batch size: 1\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "======================================== Hierarchical Summary ========================================\n",
      "\n",
      "Net(\n",
      "  (in_encoder): Sequential(\n",
      "    (0): Conv2d(1, 192, kernel_size=(3, 3), stride=(1, 1)), 1,920 params\n",
      "    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 384 params\n",
      "    (2): ReLU(), 0 params\n",
      "    (3): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1)), 37,056 params\n",
      "    (4): ReLU(), 0 params\n",
      "    (5): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2)), 331,968 params\n",
      "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 384 params\n",
      "    (7): ReLU(), 0 params\n",
      "    (8): Dropout(p=0.5, inplace=False), 0 params\n",
      "    (9): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1)), 165,984 params\n",
      "    (10): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 192 params\n",
      "    (11): ReLU(), 0 params\n",
      "    (12): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1)), 9,312 params\n",
      "    (13): ReLU(), 0 params\n",
      "    (14): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1)), 9,312 params\n",
      "    (15): ReLU(), 0 params\n",
      "    (16): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    (17): Dropout(p=0.5, inplace=False), 0 params\n",
      "    (18): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1)), 27,680 params\n",
      "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\n",
      "    (20): ReLU(), 0 params\n",
      "    (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1)), 9,248 params\n",
      "    (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\n",
      "    (23): ReLU(), 0 params\n",
      "    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)), 9,248 params\n",
      "    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\n",
      "    (26): ReLU(), 0 params\n",
      "  ), 602,880 params\n",
      "  (in_decoder): Sequential(\n",
      "    (0): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1)), 66 params\n",
      "    (1): AdaptiveAvgPool2d(output_size=(1, 1)), 0 params\n",
      "    (2): Softmax(dim=1), 0 params\n",
      "  ), 66 params\n",
      "), 602,946 params\n",
      "\n",
      "\n",
      "======================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------------------------------------------------------------\\n           Layer (type)         Output Shape         Param #     Tr. Param #\\n=============================================================================\\n               Conv2d-1     [1, 192, 62, 62]           1,920           1,920\\n          BatchNorm2d-2     [1, 192, 62, 62]             384             384\\n                 ReLU-3     [1, 192, 62, 62]               0               0\\n               Conv2d-4     [1, 192, 62, 62]          37,056          37,056\\n                 ReLU-5     [1, 192, 62, 62]               0               0\\n               Conv2d-6     [1, 192, 30, 30]         331,968         331,968\\n          BatchNorm2d-7     [1, 192, 30, 30]             384             384\\n                 ReLU-8     [1, 192, 30, 30]               0               0\\n              Dropout-9     [1, 192, 30, 30]               0               0\\n              Conv2d-10      [1, 96, 28, 28]         165,984         165,984\\n         BatchNorm2d-11      [1, 96, 28, 28]             192             192\\n                ReLU-12      [1, 96, 28, 28]               0               0\\n              Conv2d-13      [1, 96, 28, 28]           9,312           9,312\\n                ReLU-14      [1, 96, 28, 28]               0               0\\n              Conv2d-15      [1, 96, 28, 28]           9,312           9,312\\n                ReLU-16      [1, 96, 28, 28]               0               0\\n           MaxPool2d-17      [1, 96, 13, 13]               0               0\\n             Dropout-18      [1, 96, 13, 13]               0               0\\n              Conv2d-19      [1, 32, 11, 11]          27,680          27,680\\n         BatchNorm2d-20      [1, 32, 11, 11]              64              64\\n                ReLU-21      [1, 32, 11, 11]               0               0\\n              Conv2d-22        [1, 32, 9, 9]           9,248           9,248\\n         BatchNorm2d-23        [1, 32, 9, 9]              64              64\\n                ReLU-24        [1, 32, 9, 9]               0               0\\n              Conv2d-25      [1, 32, 11, 11]           9,248           9,248\\n         BatchNorm2d-26      [1, 32, 11, 11]              64              64\\n                ReLU-27      [1, 32, 11, 11]               0               0\\n              Conv2d-28       [1, 2, 11, 11]              66              66\\n   AdaptiveAvgPool2d-29         [1, 2, 1, 1]               0               0\\n             Softmax-30         [1, 2, 1, 1]               0               0\\n=============================================================================\\nTotal params: 602,946\\nTrainable params: 602,946\\nNon-trainable params: 0\\nBatch size: 1\\n-----------------------------------------------------------------------------\\n\\n\\n======================================== Hierarchical Summary ========================================\\n\\nNet(\\n  (in_encoder): Sequential(\\n    (0): Conv2d(1, 192, kernel_size=(3, 3), stride=(1, 1)), 1,920 params\\n    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 384 params\\n    (2): ReLU(), 0 params\\n    (3): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1)), 37,056 params\\n    (4): ReLU(), 0 params\\n    (5): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2)), 331,968 params\\n    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 384 params\\n    (7): ReLU(), 0 params\\n    (8): Dropout(p=0.5, inplace=False), 0 params\\n    (9): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1)), 165,984 params\\n    (10): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 192 params\\n    (11): ReLU(), 0 params\\n    (12): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1)), 9,312 params\\n    (13): ReLU(), 0 params\\n    (14): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1)), 9,312 params\\n    (15): ReLU(), 0 params\\n    (16): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\\n    (17): Dropout(p=0.5, inplace=False), 0 params\\n    (18): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1)), 27,680 params\\n    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\\n    (20): ReLU(), 0 params\\n    (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1)), 9,248 params\\n    (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\\n    (23): ReLU(), 0 params\\n    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)), 9,248 params\\n    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\\n    (26): ReLU(), 0 params\\n  ), 602,880 params\\n  (in_decoder): Sequential(\\n    (0): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1)), 66 params\\n    (1): AdaptiveAvgPool2d(output_size=(1, 1)), 0 params\\n    (2): Softmax(dim=1), 0 params\\n  ), 66 params\\n), 602,946 params\\n\\n\\n======================================================================================================\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "net = Net(encoder, decoder)\n",
    "pms.summary(net, \n",
    "    torch.zeros((1,1,64,64)), \n",
    "    batch_size=1, \n",
    "    show_hierarchical=True, \n",
    "    print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_validation(model, valloader, loss_function):\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    current_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        # Init the neural network\n",
    "        encoder = build_encoder()\n",
    "        decoder = build_decoder()\n",
    "        trained_net = Net(encoder, decoder)\n",
    "        trained_net.load_state_dict(model)\n",
    "        trained_net.to(device)\n",
    "\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "\n",
    "            # Get inputs\n",
    "            inputs = data['images']\n",
    "            targets = data['label']\n",
    "\n",
    "            outputs = None\n",
    "            for index, xbatch in enumerate(inputs):\n",
    "\n",
    "                xbatch = xbatch.float().to(device)\n",
    "                output = sliding_window(xbatch, trained_net)\n",
    "\n",
    "                # Perform forward pass\n",
    "#                 output = trained_net(xbatch)\n",
    "                if index == 0:\n",
    "                    outputs = output\n",
    "                else:\n",
    "                    outputs = torch.vstack([outputs, output])\n",
    "\n",
    "            # Compute loss\n",
    "            ybatches = targets.to(device)\n",
    "            outputs = torch.reshape(outputs, (ybatches.shape[0],NUM_CLASS))\n",
    "            ybatches.squeeze_(1)\n",
    "            loss = loss_function(outputs, ybatches)\n",
    "            current_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, truth = torch.max(ybatches, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == truth).sum().item()\n",
    "    val_loss = current_loss/(i+1)\n",
    "    val_acc = 100.0 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sliding_window(x, network):         \n",
    "    outputs = network(x)\n",
    "    outputs = torch.reshape(outputs,(-1,2))\n",
    "    final_out = torch.mean(outputs, 0)\n",
    "    return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    #---------------Config---------------\n",
    "    torch.manual_seed(42)\n",
    "    num_epochs = 50\n",
    "    k_folds = 5\n",
    "    patience = 10\n",
    "    batch_size = 32\n",
    "    #---------------Config---------------\n",
    "    \n",
    "    # Preparation\n",
    "    weight = torch.tensor([1,1]).to(device)\n",
    "    \n",
    "    loss_function = nn.BCELoss(weight=weight)\n",
    "\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "    ])\n",
    "\n",
    "    train_data_dir = './Prepared_Data/Train/'\n",
    "    train_dataset = MyDataset(root_dir=train_data_dir, transform = transformer)\n",
    "\n",
    "    val_data_dir = './Prepared_Data/Validation/'\n",
    "    val_dataset = MyDataset(root_dir=val_data_dir, transform = transformer)\n",
    "\n",
    "    dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "        training_details = []\n",
    "        best_model = None\n",
    "        print(len(train_ids))\n",
    "        print(len(val_ids))\n",
    "\n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=train_subsampler)\n",
    "        valloader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=batch_size, sampler=val_subsampler)\n",
    "\n",
    "        # Init the neural network\n",
    "        encoder = build_encoder()\n",
    "        decoder = build_decoder()\n",
    "        network = Net(encoder, decoder)\n",
    "        network.to(device) #use GPU\n",
    "        use_gpu = True\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "        max_train_acc, max_val_acc = 0, 0\n",
    "        early_stop_count = 0\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, num_epochs):\n",
    "\n",
    "            correct, total = 0, 0\n",
    "            # Print epoch\n",
    "            print(f'Starting epoch {epoch+1}')\n",
    "            # Set current loss value\n",
    "            current_loss = 0.0\n",
    "            epoch_loss = 0.0\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "            # Starting 1 cycle of mini-batch\n",
    "\n",
    "                # Get inputs\n",
    "                inputs = data['images']\n",
    "                targets = data['label']\n",
    "\n",
    "                outputs = None\n",
    "                for index, xbatch in enumerate(inputs):\n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    xbatch = xbatch.float().to(device)\n",
    "                    output = sliding_window(xbatch, network)\n",
    "\n",
    "                    # Perform forward pass\n",
    "                    if index == 0:\n",
    "                        outputs = output\n",
    "                    else:\n",
    "                        outputs = torch.vstack([outputs, output])\n",
    "\n",
    "                # Compute loss\n",
    "                ybatches = targets.to(device)\n",
    "                outputs = torch.reshape(outputs, (ybatches.shape[0],NUM_CLASS))\n",
    "                loss = loss_function(outputs, ybatches)\n",
    "\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                current_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Compute Accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, truth = torch.max(ybatches, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == truth).sum().item()\n",
    "\n",
    "            epoch_loss = epoch_loss/(i+1)\n",
    "            epoch_acc = 100.0 * correct / total\n",
    "            print('Train Loss for Epoch %d: %.4f ' % (epoch, epoch_loss))\n",
    "            print('Train Accuracy for Epoch %d: %.2f %%' % (epoch, epoch_acc))\n",
    "            val_loss, val_acc = check_validation(network.state_dict(), valloader, loss_function)\n",
    "            print('Val Loss for Epoch %d: %.4f ' % (epoch, val_loss))\n",
    "            print('Val Accuracy for Epoch %d: %.2f %%' % (epoch, val_acc))\n",
    "            print('--------------------------------')\n",
    "\n",
    "            training_details.append([epoch_acc, epoch_loss, val_acc, val_loss])\n",
    "\n",
    "            stop_improving = False\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                stop_improving = True\n",
    "                print(\"Early Stop Count Increased\")\n",
    "\n",
    "            if epoch_acc > max_train_acc:\n",
    "                max_train_acc = copy.deepcopy(epoch_acc)\n",
    "                best_model = copy.deepcopy(network.state_dict())\n",
    "                print(\"Found better model...saving\")\n",
    "\n",
    "            if early_stop_count >= patience:\n",
    "                print(\"Early stopping at Epoch\", epoch)\n",
    "                break\n",
    "\n",
    "\n",
    "        # Process is complete.\n",
    "        print('Training process has finished. Saving trained model.')\n",
    "\n",
    "        # Print about testing\n",
    "        print('Starting testing')\n",
    "\n",
    "        # Saving the model\n",
    "        save_path = f'./models/model-fold-{fold}.pth'\n",
    "        save_path_csv = f'./models/model-fold-{fold}.csv'\n",
    "        torch.save(best_model, save_path)\n",
    "\n",
    "        training_details = np.array(training_details)\n",
    "        df = {'epoch acc': training_details[:,0], \n",
    "              'epoch loss': training_details[:,1], \n",
    "              'val acc': training_details[:,2], \n",
    "              'val loss': training_details[:,3], }\n",
    "        pd.DataFrame(df).to_csv(save_path_csv)\n",
    "\n",
    "        # Evaluationfor this fold\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = check_validation(torch.load(save_path), valloader, loss_function)\n",
    "            # Print accuracy\n",
    "            print('Accuracy for fold %d: %.2f %%' % (fold, val_acc))\n",
    "            print('--------------------------------')\n",
    "            results[fold] = val_acc\n",
    "\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss for Epoch 0: 0.6937 \n",
      "Train Accuracy for Epoch 0: 50.56 %\n",
      "Val Loss for Epoch 0: 0.6870 \n",
      "Val Accuracy for Epoch 0: 50.00 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 2\n",
      "Train Loss for Epoch 1: 0.6831 \n",
      "Train Accuracy for Epoch 1: 51.40 %\n",
      "Val Loss for Epoch 1: 0.6703 \n",
      "Val Accuracy for Epoch 1: 77.78 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 3\n",
      "Train Loss for Epoch 2: 0.6668 \n",
      "Train Accuracy for Epoch 2: 80.45 %\n",
      "Val Loss for Epoch 2: 0.6492 \n",
      "Val Accuracy for Epoch 2: 91.11 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 4\n",
      "Train Loss for Epoch 3: 0.6478 \n",
      "Train Accuracy for Epoch 3: 85.47 %\n",
      "Val Loss for Epoch 3: 0.6313 \n",
      "Val Accuracy for Epoch 3: 87.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 5\n",
      "Train Loss for Epoch 4: 0.6225 \n",
      "Train Accuracy for Epoch 4: 87.99 %\n",
      "Val Loss for Epoch 4: 0.6084 \n",
      "Val Accuracy for Epoch 4: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 6\n",
      "Train Loss for Epoch 5: 0.6110 \n",
      "Train Accuracy for Epoch 5: 82.96 %\n",
      "Val Loss for Epoch 5: 0.5838 \n",
      "Val Accuracy for Epoch 5: 92.22 %\n",
      "--------------------------------\n",
      "Starting epoch 7\n",
      "Train Loss for Epoch 6: 0.5921 \n",
      "Train Accuracy for Epoch 6: 85.20 %\n",
      "Val Loss for Epoch 6: 0.5588 \n",
      "Val Accuracy for Epoch 6: 92.22 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 8\n",
      "Train Loss for Epoch 7: 0.5648 \n",
      "Train Accuracy for Epoch 7: 89.66 %\n",
      "Val Loss for Epoch 7: 0.5273 \n",
      "Val Accuracy for Epoch 7: 93.33 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 9\n",
      "Train Loss for Epoch 8: 0.5212 \n",
      "Train Accuracy for Epoch 8: 91.62 %\n",
      "Val Loss for Epoch 8: 0.4973 \n",
      "Val Accuracy for Epoch 8: 94.44 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 10\n",
      "Train Loss for Epoch 9: 0.4928 \n",
      "Train Accuracy for Epoch 9: 91.62 %\n",
      "Val Loss for Epoch 9: 0.4563 \n",
      "Val Accuracy for Epoch 9: 95.56 %\n",
      "--------------------------------\n",
      "Starting epoch 11\n",
      "Train Loss for Epoch 10: 0.4625 \n",
      "Train Accuracy for Epoch 10: 91.90 %\n",
      "Val Loss for Epoch 10: 0.4409 \n",
      "Val Accuracy for Epoch 10: 93.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 12\n",
      "Train Loss for Epoch 11: 0.4302 \n",
      "Train Accuracy for Epoch 11: 91.90 %\n",
      "Val Loss for Epoch 11: 0.3986 \n",
      "Val Accuracy for Epoch 11: 95.56 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 13\n",
      "Train Loss for Epoch 12: 0.3909 \n",
      "Train Accuracy for Epoch 12: 93.30 %\n",
      "Val Loss for Epoch 12: 0.3732 \n",
      "Val Accuracy for Epoch 12: 94.44 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 14\n",
      "Train Loss for Epoch 13: 0.3790 \n",
      "Train Accuracy for Epoch 13: 93.85 %\n",
      "Val Loss for Epoch 13: 0.3386 \n",
      "Val Accuracy for Epoch 13: 93.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 15\n",
      "Train Loss for Epoch 14: 0.3461 \n",
      "Train Accuracy for Epoch 14: 91.62 %\n",
      "Val Loss for Epoch 14: 0.3529 \n",
      "Val Accuracy for Epoch 14: 93.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 16\n",
      "Train Loss for Epoch 15: 0.3081 \n",
      "Train Accuracy for Epoch 15: 93.85 %\n",
      "Val Loss for Epoch 15: 0.2983 \n",
      "Val Accuracy for Epoch 15: 93.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 17\n",
      "Train Loss for Epoch 16: 0.2753 \n",
      "Train Accuracy for Epoch 16: 94.41 %\n",
      "Val Loss for Epoch 16: 0.2671 \n",
      "Val Accuracy for Epoch 16: 96.67 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 18\n",
      "Train Loss for Epoch 17: 0.2694 \n",
      "Train Accuracy for Epoch 17: 93.58 %\n",
      "Val Loss for Epoch 17: 0.2591 \n",
      "Val Accuracy for Epoch 17: 94.44 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 19\n",
      "Train Loss for Epoch 18: 0.2328 \n",
      "Train Accuracy for Epoch 18: 96.09 %\n",
      "Val Loss for Epoch 18: 0.2515 \n",
      "Val Accuracy for Epoch 18: 94.44 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 20\n",
      "Train Loss for Epoch 19: 0.2317 \n",
      "Train Accuracy for Epoch 19: 95.25 %\n",
      "Val Loss for Epoch 19: 0.2291 \n",
      "Val Accuracy for Epoch 19: 94.44 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 21\n",
      "Train Loss for Epoch 20: 0.2090 \n",
      "Train Accuracy for Epoch 20: 95.25 %\n",
      "Val Loss for Epoch 20: 0.2728 \n",
      "Val Accuracy for Epoch 20: 92.22 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 22\n",
      "Train Loss for Epoch 21: 0.2220 \n",
      "Train Accuracy for Epoch 21: 94.97 %\n",
      "Val Loss for Epoch 21: 0.2168 \n",
      "Val Accuracy for Epoch 21: 95.56 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 23\n",
      "Train Loss for Epoch 22: 0.1921 \n",
      "Train Accuracy for Epoch 22: 96.37 %\n",
      "Val Loss for Epoch 22: 0.2355 \n",
      "Val Accuracy for Epoch 22: 95.56 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 24\n",
      "Train Loss for Epoch 23: 0.1778 \n",
      "Train Accuracy for Epoch 23: 96.93 %\n",
      "Val Loss for Epoch 23: 0.2176 \n",
      "Val Accuracy for Epoch 23: 93.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 25\n",
      "Train Loss for Epoch 24: 0.1506 \n",
      "Train Accuracy for Epoch 24: 98.04 %\n",
      "Val Loss for Epoch 24: 0.2081 \n",
      "Val Accuracy for Epoch 24: 94.44 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 26\n",
      "Train Loss for Epoch 25: 0.1889 \n",
      "Train Accuracy for Epoch 25: 94.97 %\n",
      "Val Loss for Epoch 25: 0.2031 \n",
      "Val Accuracy for Epoch 25: 95.56 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 27\n",
      "Train Loss for Epoch 26: 0.1392 \n",
      "Train Accuracy for Epoch 26: 97.77 %\n",
      "Val Loss for Epoch 26: 0.1950 \n",
      "Val Accuracy for Epoch 26: 94.44 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Early stopping at Epoch 26\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 0: 93.33 %\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss for Epoch 0: 0.6910 \n",
      "Train Accuracy for Epoch 0: 51.96 %\n",
      "Val Loss for Epoch 0: 0.6933 \n",
      "Val Accuracy for Epoch 0: 42.22 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 2\n",
      "Train Loss for Epoch 1: 0.6702 \n",
      "Train Accuracy for Epoch 1: 60.34 %\n",
      "Val Loss for Epoch 1: 0.6680 \n",
      "Val Accuracy for Epoch 1: 80.00 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 3\n",
      "Train Loss for Epoch 2: 0.6516 \n",
      "Train Accuracy for Epoch 2: 82.96 %\n",
      "Val Loss for Epoch 2: 0.6514 \n",
      "Val Accuracy for Epoch 2: 78.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 4\n",
      "Train Loss for Epoch 3: 0.6258 \n",
      "Train Accuracy for Epoch 3: 87.43 %\n",
      "Val Loss for Epoch 3: 0.6542 \n",
      "Val Accuracy for Epoch 3: 68.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 5\n",
      "Train Loss for Epoch 4: 0.6141 \n",
      "Train Accuracy for Epoch 4: 83.80 %\n",
      "Val Loss for Epoch 4: 0.6291 \n",
      "Val Accuracy for Epoch 4: 78.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 6\n",
      "Train Loss for Epoch 5: 0.5777 \n",
      "Train Accuracy for Epoch 5: 88.55 %\n",
      "Val Loss for Epoch 5: 0.6010 \n",
      "Val Accuracy for Epoch 5: 81.11 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 7\n",
      "Train Loss for Epoch 6: 0.5452 \n",
      "Train Accuracy for Epoch 6: 88.83 %\n",
      "Val Loss for Epoch 6: 0.5693 \n",
      "Val Accuracy for Epoch 6: 83.33 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 8\n",
      "Train Loss for Epoch 7: 0.5051 \n",
      "Train Accuracy for Epoch 7: 91.90 %\n",
      "Val Loss for Epoch 7: 0.5356 \n",
      "Val Accuracy for Epoch 7: 82.22 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 9\n",
      "Train Loss for Epoch 8: 0.4642 \n",
      "Train Accuracy for Epoch 8: 94.13 %\n",
      "Val Loss for Epoch 8: 0.5445 \n",
      "Val Accuracy for Epoch 8: 76.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 10\n",
      "Train Loss for Epoch 9: 0.4221 \n",
      "Train Accuracy for Epoch 9: 95.25 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss for Epoch 9: 0.4855 \n",
      "Val Accuracy for Epoch 9: 83.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 11\n",
      "Train Loss for Epoch 10: 0.3891 \n",
      "Train Accuracy for Epoch 10: 94.41 %\n",
      "Val Loss for Epoch 10: 0.4377 \n",
      "Val Accuracy for Epoch 10: 86.67 %\n",
      "--------------------------------\n",
      "Starting epoch 12\n",
      "Train Loss for Epoch 11: 0.3521 \n",
      "Train Accuracy for Epoch 11: 94.69 %\n",
      "Val Loss for Epoch 11: 0.4493 \n",
      "Val Accuracy for Epoch 11: 83.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 13\n",
      "Train Loss for Epoch 12: 0.3250 \n",
      "Train Accuracy for Epoch 12: 94.69 %\n",
      "Val Loss for Epoch 12: 0.3993 \n",
      "Val Accuracy for Epoch 12: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 14\n",
      "Train Loss for Epoch 13: 0.2862 \n",
      "Train Accuracy for Epoch 13: 95.81 %\n",
      "Val Loss for Epoch 13: 0.4382 \n",
      "Val Accuracy for Epoch 13: 81.11 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 15\n",
      "Train Loss for Epoch 14: 0.2549 \n",
      "Train Accuracy for Epoch 14: 96.37 %\n",
      "Val Loss for Epoch 14: 0.3526 \n",
      "Val Accuracy for Epoch 14: 88.89 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 16\n",
      "Train Loss for Epoch 15: 0.2444 \n",
      "Train Accuracy for Epoch 15: 96.93 %\n",
      "Val Loss for Epoch 15: 0.3958 \n",
      "Val Accuracy for Epoch 15: 82.22 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 17\n",
      "Train Loss for Epoch 16: 0.2343 \n",
      "Train Accuracy for Epoch 16: 95.81 %\n",
      "Val Loss for Epoch 16: 0.3496 \n",
      "Val Accuracy for Epoch 16: 90.00 %\n",
      "--------------------------------\n",
      "Starting epoch 18\n",
      "Train Loss for Epoch 17: 0.2009 \n",
      "Train Accuracy for Epoch 17: 96.65 %\n",
      "Val Loss for Epoch 17: 0.4322 \n",
      "Val Accuracy for Epoch 17: 77.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 19\n",
      "Train Loss for Epoch 18: 0.1911 \n",
      "Train Accuracy for Epoch 18: 96.09 %\n",
      "Val Loss for Epoch 18: 0.3524 \n",
      "Val Accuracy for Epoch 18: 87.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 20\n",
      "Train Loss for Epoch 19: 0.2047 \n",
      "Train Accuracy for Epoch 19: 95.25 %\n",
      "Val Loss for Epoch 19: 0.3329 \n",
      "Val Accuracy for Epoch 19: 87.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 21\n",
      "Train Loss for Epoch 20: 0.1488 \n",
      "Train Accuracy for Epoch 20: 98.04 %\n",
      "Val Loss for Epoch 20: 0.3108 \n",
      "Val Accuracy for Epoch 20: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 22\n",
      "Train Loss for Epoch 21: 0.1619 \n",
      "Train Accuracy for Epoch 21: 96.37 %\n",
      "Val Loss for Epoch 21: 0.4061 \n",
      "Val Accuracy for Epoch 21: 83.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 23\n",
      "Train Loss for Epoch 22: 0.1429 \n",
      "Train Accuracy for Epoch 22: 98.04 %\n",
      "Val Loss for Epoch 22: 0.2806 \n",
      "Val Accuracy for Epoch 22: 90.00 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 24\n",
      "Train Loss for Epoch 23: 0.1276 \n",
      "Train Accuracy for Epoch 23: 98.04 %\n",
      "Val Loss for Epoch 23: 0.3041 \n",
      "Val Accuracy for Epoch 23: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 25\n",
      "Train Loss for Epoch 24: 0.1045 \n",
      "Train Accuracy for Epoch 24: 98.32 %\n",
      "Val Loss for Epoch 24: 0.2785 \n",
      "Val Accuracy for Epoch 24: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 26\n",
      "Train Loss for Epoch 25: 0.0991 \n",
      "Train Accuracy for Epoch 25: 98.60 %\n",
      "Val Loss for Epoch 25: 0.3310 \n",
      "Val Accuracy for Epoch 25: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 27\n",
      "Train Loss for Epoch 26: 0.1158 \n",
      "Train Accuracy for Epoch 26: 97.49 %\n",
      "Val Loss for Epoch 26: 0.3187 \n",
      "Val Accuracy for Epoch 26: 88.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Early stopping at Epoch 26\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 1: 85.56 %\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss for Epoch 0: 0.6900 \n",
      "Train Accuracy for Epoch 0: 49.16 %\n",
      "Val Loss for Epoch 0: 0.6885 \n",
      "Val Accuracy for Epoch 0: 46.67 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 2\n",
      "Train Loss for Epoch 1: 0.6748 \n",
      "Train Accuracy for Epoch 1: 56.15 %\n",
      "Val Loss for Epoch 1: 0.6755 \n",
      "Val Accuracy for Epoch 1: 56.67 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 3\n",
      "Train Loss for Epoch 2: 0.6553 \n",
      "Train Accuracy for Epoch 2: 65.64 %\n",
      "Val Loss for Epoch 2: 0.6567 \n",
      "Val Accuracy for Epoch 2: 66.67 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 4\n",
      "Train Loss for Epoch 3: 0.6370 \n",
      "Train Accuracy for Epoch 3: 78.49 %\n",
      "Val Loss for Epoch 3: 0.6351 \n",
      "Val Accuracy for Epoch 3: 83.33 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 5\n",
      "Train Loss for Epoch 4: 0.6159 \n",
      "Train Accuracy for Epoch 4: 87.99 %\n",
      "Val Loss for Epoch 4: 0.6070 \n",
      "Val Accuracy for Epoch 4: 86.67 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 6\n",
      "Train Loss for Epoch 5: 0.5781 \n",
      "Train Accuracy for Epoch 5: 91.06 %\n",
      "Val Loss for Epoch 5: 0.5770 \n",
      "Val Accuracy for Epoch 5: 87.78 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 7\n",
      "Train Loss for Epoch 6: 0.5455 \n",
      "Train Accuracy for Epoch 6: 92.18 %\n",
      "Val Loss for Epoch 6: 0.5427 \n",
      "Val Accuracy for Epoch 6: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 8\n",
      "Train Loss for Epoch 7: 0.4990 \n",
      "Train Accuracy for Epoch 7: 93.30 %\n",
      "Val Loss for Epoch 7: 0.5394 \n",
      "Val Accuracy for Epoch 7: 83.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 9\n",
      "Train Loss for Epoch 8: 0.4793 \n",
      "Train Accuracy for Epoch 8: 91.34 %\n",
      "Val Loss for Epoch 8: 0.4880 \n",
      "Val Accuracy for Epoch 8: 91.11 %\n",
      "--------------------------------\n",
      "Starting epoch 10\n",
      "Train Loss for Epoch 9: 0.4357 \n",
      "Train Accuracy for Epoch 9: 93.58 %\n",
      "Val Loss for Epoch 9: 0.4770 \n",
      "Val Accuracy for Epoch 9: 85.56 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 11\n",
      "Train Loss for Epoch 10: 0.3898 \n",
      "Train Accuracy for Epoch 10: 93.85 %\n",
      "Val Loss for Epoch 10: 0.4127 \n",
      "Val Accuracy for Epoch 10: 88.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 12\n",
      "Train Loss for Epoch 11: 0.3470 \n",
      "Train Accuracy for Epoch 11: 95.53 %\n",
      "Val Loss for Epoch 11: 0.4167 \n",
      "Val Accuracy for Epoch 11: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 13\n",
      "Train Loss for Epoch 12: 0.3164 \n",
      "Train Accuracy for Epoch 12: 94.97 %\n",
      "Val Loss for Epoch 12: 0.3662 \n",
      "Val Accuracy for Epoch 12: 87.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 14\n",
      "Train Loss for Epoch 13: 0.2771 \n",
      "Train Accuracy for Epoch 13: 96.65 %\n",
      "Val Loss for Epoch 13: 0.3511 \n",
      "Val Accuracy for Epoch 13: 88.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 15\n",
      "Train Loss for Epoch 14: 0.2522 \n",
      "Train Accuracy for Epoch 14: 95.81 %\n",
      "Val Loss for Epoch 14: 0.3916 \n",
      "Val Accuracy for Epoch 14: 87.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 16\n",
      "Train Loss for Epoch 15: 0.2422 \n",
      "Train Accuracy for Epoch 15: 96.37 %\n",
      "Val Loss for Epoch 15: 0.3509 \n",
      "Val Accuracy for Epoch 15: 87.78 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 17\n",
      "Train Loss for Epoch 16: 0.2260 \n",
      "Train Accuracy for Epoch 16: 95.53 %\n",
      "Val Loss for Epoch 16: 0.3707 \n",
      "Val Accuracy for Epoch 16: 83.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 18\n",
      "Train Loss for Epoch 17: 0.2174 \n",
      "Train Accuracy for Epoch 17: 95.81 %\n",
      "Val Loss for Epoch 17: 0.3703 \n",
      "Val Accuracy for Epoch 17: 83.33 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 19\n",
      "Train Loss for Epoch 18: 0.1941 \n",
      "Train Accuracy for Epoch 18: 97.21 %\n",
      "Val Loss for Epoch 18: 0.3221 \n",
      "Val Accuracy for Epoch 18: 86.67 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Early stopping at Epoch 18\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 2: 88.89 %\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss for Epoch 0: 0.6932 \n",
      "Train Accuracy for Epoch 0: 50.14 %\n",
      "Val Loss for Epoch 0: 0.6868 \n",
      "Val Accuracy for Epoch 0: 49.44 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 2\n",
      "Train Loss for Epoch 1: 0.6828 \n",
      "Train Accuracy for Epoch 1: 53.48 %\n",
      "Val Loss for Epoch 1: 0.6723 \n",
      "Val Accuracy for Epoch 1: 73.03 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 3\n",
      "Train Loss for Epoch 2: 0.6677 \n",
      "Train Accuracy for Epoch 2: 71.03 %\n",
      "Val Loss for Epoch 2: 0.6564 \n",
      "Val Accuracy for Epoch 2: 83.15 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 4\n",
      "Train Loss for Epoch 3: 0.6494 \n",
      "Train Accuracy for Epoch 3: 85.24 %\n",
      "Val Loss for Epoch 3: 0.6337 \n",
      "Val Accuracy for Epoch 3: 88.76 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 5\n",
      "Train Loss for Epoch 4: 0.6322 \n",
      "Train Accuracy for Epoch 4: 87.19 %\n",
      "Val Loss for Epoch 4: 0.6137 \n",
      "Val Accuracy for Epoch 4: 92.13 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 6\n",
      "Train Loss for Epoch 5: 0.6115 \n",
      "Train Accuracy for Epoch 5: 89.97 %\n",
      "Val Loss for Epoch 5: 0.6046 \n",
      "Val Accuracy for Epoch 5: 88.76 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 7\n",
      "Train Loss for Epoch 6: 0.5889 \n",
      "Train Accuracy for Epoch 6: 88.58 %\n",
      "Val Loss for Epoch 6: 0.5652 \n",
      "Val Accuracy for Epoch 6: 93.26 %\n",
      "--------------------------------\n",
      "Starting epoch 8\n",
      "Train Loss for Epoch 7: 0.5524 \n",
      "Train Accuracy for Epoch 7: 91.36 %\n",
      "Val Loss for Epoch 7: 0.5445 \n",
      "Val Accuracy for Epoch 7: 86.52 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 9\n",
      "Train Loss for Epoch 8: 0.5180 \n",
      "Train Accuracy for Epoch 8: 91.64 %\n",
      "Val Loss for Epoch 8: 0.4949 \n",
      "Val Accuracy for Epoch 8: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 10\n",
      "Train Loss for Epoch 9: 0.4718 \n",
      "Train Accuracy for Epoch 9: 93.04 %\n",
      "Val Loss for Epoch 9: 0.4874 \n",
      "Val Accuracy for Epoch 9: 86.52 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 11\n",
      "Train Loss for Epoch 10: 0.4483 \n",
      "Train Accuracy for Epoch 10: 91.36 %\n",
      "Val Loss for Epoch 10: 0.4393 \n",
      "Val Accuracy for Epoch 10: 88.76 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 12\n",
      "Train Loss for Epoch 11: 0.4113 \n",
      "Train Accuracy for Epoch 11: 91.64 %\n",
      "Val Loss for Epoch 11: 0.4037 \n",
      "Val Accuracy for Epoch 11: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 13\n",
      "Train Loss for Epoch 12: 0.3428 \n",
      "Train Accuracy for Epoch 12: 95.82 %\n",
      "Val Loss for Epoch 12: 0.3648 \n",
      "Val Accuracy for Epoch 12: 91.01 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 14\n",
      "Train Loss for Epoch 13: 0.3318 \n",
      "Train Accuracy for Epoch 13: 94.99 %\n",
      "Val Loss for Epoch 13: 0.3639 \n",
      "Val Accuracy for Epoch 13: 87.64 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 15\n",
      "Train Loss for Epoch 14: 0.2903 \n",
      "Train Accuracy for Epoch 14: 95.26 %\n",
      "Val Loss for Epoch 14: 0.3946 \n",
      "Val Accuracy for Epoch 14: 86.52 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 16\n",
      "Train Loss for Epoch 15: 0.2801 \n",
      "Train Accuracy for Epoch 15: 94.15 %\n",
      "Val Loss for Epoch 15: 0.3505 \n",
      "Val Accuracy for Epoch 15: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 17\n",
      "Train Loss for Epoch 16: 0.3005 \n",
      "Train Accuracy for Epoch 16: 92.48 %\n",
      "Val Loss for Epoch 16: 0.3642 \n",
      "Val Accuracy for Epoch 16: 85.39 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Early stopping at Epoch 16\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 3: 89.89 %\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss for Epoch 0: 0.6948 \n",
      "Train Accuracy for Epoch 0: 50.97 %\n",
      "Val Loss for Epoch 0: 0.6878 \n",
      "Val Accuracy for Epoch 0: 46.07 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 2\n",
      "Train Loss for Epoch 1: 0.6743 \n",
      "Train Accuracy for Epoch 1: 53.48 %\n",
      "Val Loss for Epoch 1: 0.6712 \n",
      "Val Accuracy for Epoch 1: 64.04 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 3\n",
      "Train Loss for Epoch 2: 0.6592 \n",
      "Train Accuracy for Epoch 2: 67.41 %\n",
      "Val Loss for Epoch 2: 0.6554 \n",
      "Val Accuracy for Epoch 2: 74.16 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 4\n",
      "Train Loss for Epoch 3: 0.6385 \n",
      "Train Accuracy for Epoch 3: 84.40 %\n",
      "Val Loss for Epoch 3: 0.6338 \n",
      "Val Accuracy for Epoch 3: 80.90 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 5\n",
      "Train Loss for Epoch 4: 0.6174 \n",
      "Train Accuracy for Epoch 4: 88.02 %\n",
      "Val Loss for Epoch 4: 0.6093 \n",
      "Val Accuracy for Epoch 4: 87.64 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 6\n",
      "Train Loss for Epoch 5: 0.5966 \n",
      "Train Accuracy for Epoch 5: 88.86 %\n",
      "Val Loss for Epoch 5: 0.5880 \n",
      "Val Accuracy for Epoch 5: 91.01 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 7\n",
      "Train Loss for Epoch 6: 0.5685 \n",
      "Train Accuracy for Epoch 6: 89.69 %\n",
      "Val Loss for Epoch 6: 0.5621 \n",
      "Val Accuracy for Epoch 6: 91.01 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 8\n",
      "Train Loss for Epoch 7: 0.5433 \n",
      "Train Accuracy for Epoch 7: 91.36 %\n",
      "Val Loss for Epoch 7: 0.5363 \n",
      "Val Accuracy for Epoch 7: 93.26 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 9\n",
      "Train Loss for Epoch 8: 0.5094 \n",
      "Train Accuracy for Epoch 8: 92.48 %\n",
      "Val Loss for Epoch 8: 0.5105 \n",
      "Val Accuracy for Epoch 8: 93.26 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 10\n",
      "Train Loss for Epoch 9: 0.4802 \n",
      "Train Accuracy for Epoch 9: 91.36 %\n",
      "Val Loss for Epoch 9: 0.4712 \n",
      "Val Accuracy for Epoch 9: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 11\n",
      "Train Loss for Epoch 10: 0.4496 \n",
      "Train Accuracy for Epoch 10: 91.92 %\n",
      "Val Loss for Epoch 10: 0.4327 \n",
      "Val Accuracy for Epoch 10: 94.38 %\n",
      "--------------------------------\n",
      "Starting epoch 12\n",
      "Train Loss for Epoch 11: 0.4026 \n",
      "Train Accuracy for Epoch 11: 93.31 %\n",
      "Val Loss for Epoch 11: 0.4061 \n",
      "Val Accuracy for Epoch 11: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 13\n",
      "Train Loss for Epoch 12: 0.3876 \n",
      "Train Accuracy for Epoch 12: 93.59 %\n",
      "Val Loss for Epoch 12: 0.3795 \n",
      "Val Accuracy for Epoch 12: 92.13 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 14\n",
      "Train Loss for Epoch 13: 0.3624 \n",
      "Train Accuracy for Epoch 13: 93.59 %\n",
      "Val Loss for Epoch 13: 0.3547 \n",
      "Val Accuracy for Epoch 13: 94.38 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 15\n",
      "Train Loss for Epoch 14: 0.3540 \n",
      "Train Accuracy for Epoch 14: 93.31 %\n",
      "Val Loss for Epoch 14: 0.3410 \n",
      "Val Accuracy for Epoch 14: 94.38 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 16\n",
      "Train Loss for Epoch 15: 0.3019 \n",
      "Train Accuracy for Epoch 15: 94.43 %\n",
      "Val Loss for Epoch 15: 0.2951 \n",
      "Val Accuracy for Epoch 15: 95.51 %\n",
      "--------------------------------\n",
      "Found better model...saving\n",
      "Starting epoch 17\n",
      "Train Loss for Epoch 16: 0.2790 \n",
      "Train Accuracy for Epoch 16: 94.43 %\n",
      "Val Loss for Epoch 16: 0.2785 \n",
      "Val Accuracy for Epoch 16: 93.26 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 18\n",
      "Train Loss for Epoch 17: 0.2540 \n",
      "Train Accuracy for Epoch 17: 95.54 %\n",
      "Val Loss for Epoch 17: 0.2772 \n",
      "Val Accuracy for Epoch 17: 94.38 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 19\n",
      "Train Loss for Epoch 18: 0.2297 \n",
      "Train Accuracy for Epoch 18: 95.54 %\n",
      "Val Loss for Epoch 18: 0.2522 \n",
      "Val Accuracy for Epoch 18: 95.51 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 20\n",
      "Train Loss for Epoch 19: 0.2430 \n",
      "Train Accuracy for Epoch 19: 95.82 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss for Epoch 19: 0.2539 \n",
      "Val Accuracy for Epoch 19: 94.38 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 21\n",
      "Train Loss for Epoch 20: 0.1867 \n",
      "Train Accuracy for Epoch 20: 97.49 %\n",
      "Val Loss for Epoch 20: 0.2505 \n",
      "Val Accuracy for Epoch 20: 93.26 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 22\n",
      "Train Loss for Epoch 21: 0.1633 \n",
      "Train Accuracy for Epoch 21: 97.49 %\n",
      "Val Loss for Epoch 21: 0.2648 \n",
      "Val Accuracy for Epoch 21: 93.26 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 23\n",
      "Train Loss for Epoch 22: 0.1412 \n",
      "Train Accuracy for Epoch 22: 98.05 %\n",
      "Val Loss for Epoch 22: 0.2455 \n",
      "Val Accuracy for Epoch 22: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Found better model...saving\n",
      "Starting epoch 24\n",
      "Train Loss for Epoch 23: 0.1638 \n",
      "Train Accuracy for Epoch 23: 97.49 %\n",
      "Val Loss for Epoch 23: 0.3131 \n",
      "Val Accuracy for Epoch 23: 89.89 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 25\n",
      "Train Loss for Epoch 24: 0.1754 \n",
      "Train Accuracy for Epoch 24: 97.21 %\n",
      "Val Loss for Epoch 24: 0.2322 \n",
      "Val Accuracy for Epoch 24: 92.13 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Starting epoch 26\n",
      "Train Loss for Epoch 25: 0.1401 \n",
      "Train Accuracy for Epoch 25: 96.66 %\n",
      "Val Loss for Epoch 25: 0.2327 \n",
      "Val Accuracy for Epoch 25: 94.38 %\n",
      "--------------------------------\n",
      "Early Stop Count Increased\n",
      "Early stopping at Epoch 25\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 4: 96.63 %\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 93.33333333333333 %\n",
      "Fold 1: 85.55555555555556 %\n",
      "Fold 2: 88.88888888888889 %\n",
      "Fold 3: 89.88764044943821 %\n",
      "Fold 4: 96.62921348314607 %\n",
      "Average: 90.85892634207241 %\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    avg_acc = 0\n",
    "    avg_f1 = 0\n",
    "    avg_conf = np.zeros((2,2))\n",
    "    \n",
    "    # Preparation\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "    ])\n",
    "    test_data_dir = './Prepared_Data/Test/'\n",
    "    test_dataset = MyDataset(root_dir=test_data_dir, transform = transformer)\n",
    "\n",
    "    for i in range(5):\n",
    "        PATH = './Plan1_Models/best/model-fold-' + str(i) + '.pth'\n",
    "        encoder = build_encoder()\n",
    "        decoder = build_decoder()\n",
    "        trained_net = Net(encoder,decoder)\n",
    "        trained_net.load_state_dict(torch.load(PATH))\n",
    "        trained_net.to(device)\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_true, y_pred = [], []\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size = 50)\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the test data and generate predictions\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "\n",
    "                # Get inputs\n",
    "                inputs = data['images']\n",
    "                targets = data['label']\n",
    "\n",
    "                outputs = None\n",
    "                for index, xbatch in enumerate(inputs):\n",
    "\n",
    "                    xbatch = xbatch.float().to(device)\n",
    "\n",
    "                    # Perform forward pass\n",
    "                    output = sliding_window(xbatch, trained_net)\n",
    "                    if index == 0:\n",
    "                        outputs = output\n",
    "                    else:\n",
    "                        outputs = torch.vstack([outputs, output])\n",
    "\n",
    "                # Compute loss\n",
    "                ybatches = targets.to(device)\n",
    "                outputs = torch.reshape(outputs, (ybatches.shape[0],NUM_CLASS))\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, truth = torch.max(ybatches, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == truth).sum().item()\n",
    "\n",
    "                y_pred.extend(predicted.cpu())\n",
    "                y_true.extend(truth.cpu())\n",
    "\n",
    "        # Print accuracy\n",
    "        print(\"Accuracy\", 100.0 * correct / total)\n",
    "        f1 = f1_score(y_true, y_pred, labels=[0,1], average = 'binary')\n",
    "        confusion_matr = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        print(\"F1\", f1)\n",
    "        print(confusion_matr)\n",
    "\n",
    "        avg_acc += 100.0 * correct / total\n",
    "        avg_f1 += f1\n",
    "        np.add(avg_conf, np.array(confusion_matr))\n",
    "\n",
    "    avg_acc = avg_acc/5\n",
    "    avg_f1 = avg_f1/5\n",
    "    np.true_divide(avg_conf, 5)\n",
    "    print(\"--------------Average------------\")\n",
    "    print('Avg Accuracy', avg_acc)\n",
    "    print('Avg F1', avg_f1)\n",
    "    print('Avg Confusion', avg_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 70.3125\n",
      "F1 0.7710843373493976\n",
      "[[13 19]\n",
      " [ 0 32]]\n",
      "Accuracy 73.4375\n",
      "F1 0.767123287671233\n",
      "[[19 13]\n",
      " [ 4 28]]\n",
      "Accuracy 75.0\n",
      "F1 0.7948717948717949\n",
      "[[17 15]\n",
      " [ 1 31]]\n",
      "Accuracy 81.25\n",
      "F1 0.8421052631578948\n",
      "[[20 12]\n",
      " [ 0 32]]\n",
      "Accuracy 87.5\n",
      "F1 0.8823529411764706\n",
      "[[26  6]\n",
      " [ 2 30]]\n",
      "--------------Average------------\n",
      "Avg Accuracy 77.5\n",
      "Avg F1 0.8115075248453583\n",
      "Avg Confusion [[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
